{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Pipeline For The Titanic Dataset\n",
    "\n",
    "## Introduction\n",
    "[Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/) helps with building entire workflows\n",
    "\n",
    "These steps can be triggered automatically by a CI/CD workflow or on demand from a command line or notebook.\n",
    "\n",
    "\n",
    "**Components** performs a single step in a Machine Learning workflow such (e.g. data ingestion, data preprocessing, data transformation, model training, hyperparameter tuning).\n",
    "\n",
    "**Dataset:** Titanic dataset, contains data collated from the shipâ€™s manifest on its passengers. There are 1309 rows of data segmented in train and test files.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "check to see if kfp is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 show kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure access Minio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload our Training Dataset to Minio\n",
    "\n",
    "First, we configure credentials for `mc`, the MinIO command line client.\n",
    "We then use it to create a bucket, upload the dataset to it, and set access policy so that the pipeline can download it from MinIO.\n",
    "\n",
    "Follow the steps below to download minio client\n",
    "<div class=\"alert\">\n",
    "   <code>\n",
    "    wget https://dl.min.io/client/mc/release/linux-amd64/mc\n",
    "    chmod +x mc\n",
    "    ./mc --help\n",
    "    </code>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://dl.min.io/client/mc/release/linux-amd64/mc\n",
    "! chmod +x mc\n",
    "! ./mc --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Connect to the Minio Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./mc alias set minio http://minio-service.kubeflow:9000 minio minio123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Create a bucket to stor your data and export your model to Minio\n",
    "\n",
    "**Make sure you clear this bucket once you are cone running your pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./mc mb minio/titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./mc ls minio mlpipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Upload the dataset to your bucket in Minio.\n",
    "\n",
    "**Note**: Make sure you have your dataset in a folder like we have here as <code>datasets</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar --dereference -czf datasets.tar.gz ./dataset\n",
    "! ./mc cp datasets.tar.gz minio/titanic/dataset.tar.gz\n",
    "! ./mc policy set download minio/titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you have downloaded your data too many times while testing, use the following code to clear out your bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ./mc rm --recursive --force minio/titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minio Server URL and Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_SERVER='minio-service.kubeflow:9000'\n",
    "MINIO_ACCESS_KEY='minio'\n",
    "MINIO_SECRET_KEY='minio123'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Implement Kubeflow Pipelines Components\n",
    "\n",
    "In this pipeline, we have the following components:\n",
    "- Titanic dataset download component\n",
    "- Preprocess the dataset component\n",
    "- Feature Engineering for the Titanic DataSet\n",
    "- Train the models component\n",
    "- Export the trained model component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import kfp\n",
    "import kfp.components as components\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import InputPath, OutputPath #helps define the input & output between the components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 1: Download the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(minio_server: str,data_dir: OutputPath(str)):\n",
    "\n",
    "    import urllib.request\n",
    "    import tarfile\n",
    "    import os\n",
    "    import subprocess\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    url = f'http://{minio_server}/titanic/dataset.tar.gz'\n",
    "    stream = urllib.request.urlopen(url)\n",
    "    tar = tarfile.open(fileobj=stream, mode=\"r|gz\")\n",
    "    tar.extractall(path=data_dir)\n",
    "    \n",
    "\n",
    "    subprocess.call([\"ls\", \"-dlha\", data_dir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 2: Preprocess the Titanic DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(data_dir: InputPath(str), preprocessed_data_dir: OutputPath(str)):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import re\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import subprocess\n",
    "    \n",
    "    \n",
    "    train_df = pd.read_csv(f\"{data_dir}/dataset/train.csv\")\n",
    "    test_df= pd.read_csv(f\"{data_dir}/dataset/test.csv\")\n",
    "    \n",
    "    data = [train_df, test_df]\n",
    "    for dataset in data:\n",
    "        dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n",
    "        dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n",
    "        dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n",
    "        dataset['not_alone'] = dataset['not_alone'].astype(int)\n",
    "        \n",
    "    # This does not contribute to a person survival probability\n",
    "    train_df = train_df.drop(['PassengerId'], axis=1)\n",
    "   \n",
    "    #dealing with missing data in cabin feature\n",
    "    deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n",
    "    data = [train_df, test_df]\n",
    "\n",
    "    for dataset in data:\n",
    "        dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n",
    "        dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "        dataset['Deck'] = dataset['Deck'].map(deck)\n",
    "        dataset['Deck'] = dataset['Deck'].fillna(0)\n",
    "        dataset['Deck'] = dataset['Deck'].astype(int)\n",
    "    # we can now drop the cabin feature\n",
    "    train_df = train_df.drop(['Cabin'], axis=1)\n",
    "    test_df = test_df.drop(['Cabin'], axis=1)\n",
    "    \n",
    "    #dealing with missing data in age feature\n",
    "    data = [train_df, test_df]\n",
    "    \n",
    "    for dataset in data:\n",
    "        mean = train_df[\"Age\"].mean()\n",
    "        std = test_df[\"Age\"].std()\n",
    "        is_null = dataset[\"Age\"].isnull().sum()\n",
    "        # compute random numbers between the mean, std and is_null\n",
    "        rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n",
    "        # fill NaN values in Age column with random values generated\n",
    "        age_slice = dataset[\"Age\"].copy()\n",
    "        age_slice[np.isnan(age_slice)] = rand_age\n",
    "        dataset[\"Age\"] = age_slice\n",
    "        dataset[\"Age\"] = train_df[\"Age\"].astype(int)\n",
    "\n",
    "    #dealing with missing data in emabrk feature\n",
    "    # fill with most common value\n",
    "    common_value = 'S'\n",
    "    data = [train_df, test_df]\n",
    "\n",
    "    for dataset in data:\n",
    "        dataset['Embarked'] = dataset['Embarked'].fillna(common_value)\n",
    "    \n",
    "    os.makedirs(preprocessed_data_dir, exist_ok=True)\n",
    "                         \n",
    "    train_df.to_pickle(f'{preprocessed_data_dir}/train.pkl')\n",
    "    test_df.to_pickle(f'{preprocessed_data_dir}/test.pkl')\n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 3: Feature Engineering for the Titanic DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feateng_dataset(preprocessed_data_dir: InputPath(str), feature_dir: OutputPath(str)):\n",
    "        \n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import re\n",
    "    import numpy as np\n",
    "    import os\n",
    "    \n",
    "    #loading the preprocessed data\n",
    "    train_df = pd.read_pickle(f'{preprocessed_data_dir}/train.pkl')\n",
    "    test_df = pd.read_pickle(f'{preprocessed_data_dir}/test.pkl')\n",
    "    \n",
    "    \n",
    "    data = [train_df, test_df]\n",
    "    for dataset in data:\n",
    "        dataset['Fare'] = dataset['Fare'].fillna(0)\n",
    "        dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "        \n",
    "    #title features\n",
    "    data = [train_df, test_df]\n",
    "    titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "\n",
    "    for dataset in data:\n",
    "        # extract titles\n",
    "        dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "        # replace titles with a more common title or as Rare\n",
    "        dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n",
    "                                                'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "        dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "        dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "        dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "        # convert titles into numbers\n",
    "        dataset['Title'] = dataset['Title'].map(titles)\n",
    "        # filling NaN with 0, to get safe\n",
    "        dataset['Title'] = dataset['Title'].fillna(0)\n",
    "    train_df = train_df.drop(['Name'], axis=1)\n",
    "    test_df = test_df.drop(['Name'], axis=1)\n",
    "    \n",
    "    #mapping sex feature into numeric\n",
    "    genders = {\"male\": 0, \"female\": 1}\n",
    "    data = [train_df, test_df]\n",
    "\n",
    "    for dataset in data:\n",
    "        dataset['Sex'] = dataset['Sex'].map(genders)\n",
    "    \n",
    "    #dropping ticket feature\n",
    "    train_df = train_df.drop(['Ticket'], axis=1)\n",
    "    test_df = test_df.drop(['Ticket'], axis=1)\n",
    "    \n",
    "    #mapping embarked into numeric\n",
    "    ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\n",
    "    data = [train_df, test_df]\n",
    "\n",
    "    for dataset in data:\n",
    "        dataset['Embarked'] = dataset['Embarked'].map(ports)\n",
    "      \n",
    "    #grouping age into categories\n",
    "    data = [train_df, test_df]\n",
    "    for dataset in data:\n",
    "        dataset['Age'] = dataset['Age'].astype(int)\n",
    "        dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n",
    "        dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n",
    "        dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n",
    "        dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n",
    "        dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n",
    "        dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n",
    "        dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n",
    "        dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n",
    "     \n",
    "    #grouping fare into categories\n",
    "    data = [train_df, test_df]\n",
    "\n",
    "    for dataset in data:\n",
    "        dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
    "        dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "        dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "        dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n",
    "        dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n",
    "        dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n",
    "        dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "        \n",
    "    #adding new feature\n",
    "    #age times class\n",
    "    data = [train_df, test_df]\n",
    "    for dataset in data:\n",
    "        dataset['Age_Class']= dataset['Age']* dataset['Pclass']\n",
    "    #fare per head    \n",
    "    for dataset in data:\n",
    "        \n",
    "        dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['relatives']+1)\n",
    "        dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n",
    "    \n",
    "    X_train = train_df.drop(\"Survived\",axis=1)\n",
    "    Y_train = train_df[\"Survived\"]\n",
    "    X_test  = test_df.drop(\"PassengerId\",axis=1)\n",
    "    X_test  = X_test.copy()\n",
    "    \n",
    "    #Save the train_data as a pickle file to be used by the train component.\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "    with open(f'{feature_dir}/train', 'wb') as f:\n",
    "        pickle.dump((X_train,  Y_train), f)\n",
    "    \n",
    "    #Save the test_feature as a pickle file to be used.\n",
    "    with open(f'{feature_dir}/test', 'wb') as f:\n",
    "        pickle.dump(X_test, f)\n",
    "        \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 4: Train the Model\n",
    "\n",
    "#### a. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_reg(feature_dir: InputPath(str), l_models_dir: OutputPath(str)):\n",
    "    \n",
    "    import pickle\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import linear_model\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    #loading the train data\n",
    "    with open(f'{feature_dir}/train', 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    # Separate the X_train from y_train.\n",
    "    X_train, Y_train = train_data\n",
    "    \n",
    "    logreg = LogisticRegression(solver='lbfgs', max_iter=110)\n",
    "    logreg.fit(X_train, Y_train)\n",
    "    acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n",
    "    \n",
    "    os.makedirs(l_models_dir, exist_ok=True)\n",
    "    #Save the accuracy as a pickle file to be used \n",
    "    with open(f'{l_models_dir}/logistic_reg', 'wb') as f:\n",
    "        pickle.dump(acc_log, f)\n",
    "    \n",
    "    return(print('Done!'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_NB(feature_dir: InputPath(str), g_models_dir: OutputPath(str)):\n",
    "    \n",
    "    import pickle\n",
    "    from sklearn import linear_model\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    #loading the train data\n",
    "    with open(f'{feature_dir}/train', 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    # Separate the X_train from y_train.\n",
    "    X_train, Y_train = train_data\n",
    "    \n",
    "    gaussian = GaussianNB()\n",
    "    gaussian.fit(X_train, Y_train)\n",
    "    acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\n",
    "    \n",
    "    os.makedirs(g_models_dir, exist_ok=True)\n",
    "    #Save the accuracy as a pickle file to be used \n",
    "    with open(f'{g_models_dir}/gaus_NB', 'wb') as f:\n",
    "        pickle.dump(acc_gaussian, f)\n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(feature_dir: InputPath(str), s_models_dir: OutputPath(str)):\n",
    "    import pickle\n",
    "    from sklearn import linear_model\n",
    "    from sklearn.svm import SVC\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    #loading the train data\n",
    "    with open(f'{feature_dir}/train', 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    # Separate the X_train from y_train.\n",
    "    X_train, Y_train = train_data\n",
    "    \n",
    "    linear_svc = SVC(gamma='auto')\n",
    "    linear_svc.fit(X_train, Y_train)\n",
    "    acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n",
    "    \n",
    "    os.makedirs(s_models_dir, exist_ok=True)\n",
    "    #Save the accuracy as a pickle file to be used \n",
    "    with open(f'{s_models_dir}/svm', 'wb') as f:\n",
    "        pickle.dump(acc_linear_svc, f)\n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(feature_dir: InputPath(str), d_models_dir: OutputPath(str)):\n",
    "    \n",
    "    import pickle\n",
    "    from sklearn import linear_model\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    #loading the train data\n",
    "    with open(f'{feature_dir}/train', 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    # Separate the X_train from y_train.\n",
    "    X_train, Y_train = train_data\n",
    "    \n",
    "    decision_tree = DecisionTreeClassifier()\n",
    "    decision_tree.fit(X_train, Y_train)\n",
    "    acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n",
    "    \n",
    "    os.makedirs(d_models_dir, exist_ok=True)\n",
    "    #Save the accuracy as a pickle file to be used \n",
    "    with open(f'{d_models_dir}/decision_tree', 'wb') as f:\n",
    "        pickle.dump(acc_decision_tree, f)\n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 4: Export the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(\n",
    "    l_models_dir: InputPath(str),\n",
    "    g_models_dir: InputPath(str),\n",
    "    s_models_dir: InputPath(str),\n",
    "    d_models_dir: InputPath(str),\n",
    "    export_bucket: str,\n",
    "    minio_server: str,\n",
    "    minio_access_key: str,\n",
    "    minio_secret_key: str,\n",
    "):\n",
    "    import os\n",
    "    import boto3\n",
    "    from botocore.client import Config\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=f'http://{minio_server}',\n",
    "        aws_access_key_id=minio_access_key,\n",
    "        aws_secret_access_key=minio_secret_key,\n",
    "        config=Config(signature_version=\"s3v4\"),\n",
    "    )\n",
    "\n",
    "    # Create export bucket if it does not yet exist\n",
    "    response = s3.list_buckets()\n",
    "    export_bucket_exists = False\n",
    "\n",
    "    for bucket in response[\"Buckets\"]:\n",
    "        if bucket[\"Name\"] == export_bucket:\n",
    "            export_bucket_exists = True\n",
    "\n",
    "    if not export_bucket_exists:\n",
    "        s3.create_bucket(ACL=\"public-read-write\", Bucket=export_bucket)\n",
    "\n",
    "    # Save model files to S3\n",
    "    models_name = []\n",
    "    for i in os.listdir(l_models_dir):\n",
    "        models_name.append(i)\n",
    "    for root, dirs, files in os.walk(l_models_dir):\n",
    "        for filename in files:\n",
    "            local_path = os.path.join(root, filename)\n",
    "            s3_path = os.path.relpath(local_path, l_models_dir)\n",
    "\n",
    "            s3.upload_file(\n",
    "                local_path,\n",
    "                export_bucket,\n",
    "                f\"{s3_path}\",\n",
    "                ExtraArgs={\"ACL\": \"public-read\"},\n",
    "            )\n",
    "\n",
    "    response = s3.list_objects(Bucket=export_bucket)\n",
    "    print(f\"All objects in {export_bucket}:\")\n",
    "    for file in response[\"Contents\"]:\n",
    "        print(\"{}/{}\".format(export_bucket, file[\"Key\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the Components into a Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_pipeline(\n",
    "    data_dir: str,\n",
    "    preprocessed_data_dir: str,\n",
    "    feature_dir: str,\n",
    "    l_models_dir: str,\n",
    "    g_models_dir: str,\n",
    "    s_models_dir: str,\n",
    "    d_models_dir: str,\n",
    "    export_bucket: str,\n",
    "    minio_server: str,\n",
    "    minio_access_key: str,\n",
    "    minio_secret_key: str,\n",
    "):\n",
    "    # For GPU support, please add the \"-gpu\" suffix to the base image\n",
    "    BASE_IMAGE = \"mavencodev/minio:v.0.1\"\n",
    "\n",
    "    downloadOp = components.func_to_container_op(\n",
    "        download_dataset, base_image=BASE_IMAGE\n",
    "    )(minio_server)\n",
    "\n",
    "    preprocessOp = components.func_to_container_op(preprocess_dataset,base_image=BASE_IMAGE)(\n",
    "        downloadOp.output\n",
    "    )\n",
    "    \n",
    "    featureOp = components.func_to_container_op(feateng_dataset, base_image=BASE_IMAGE)(\n",
    "        preprocessOp.output\n",
    "    )\n",
    "    \n",
    "    \n",
    "    logistic_regOp = components.func_to_container_op(logistic_reg, base_image=BASE_IMAGE)(\n",
    "        featureOp.output\n",
    "    )\n",
    "    \n",
    "    gaussian_NB_Op = components.func_to_container_op(gaussian_NB, base_image=BASE_IMAGE)(\n",
    "        featureOp.output\n",
    "    )\n",
    "    \n",
    "    svmOp = components.func_to_container_op(SVM, base_image=BASE_IMAGE)(\n",
    "        featureOp.output\n",
    "    )\n",
    "    \n",
    "    decision_treesOp = components.func_to_container_op(decision_tree, base_image=BASE_IMAGE)(\n",
    "        featureOp.output\n",
    "    )\n",
    "\n",
    "    exportOp = components.func_to_container_op(export_model, base_image=BASE_IMAGE)(\n",
    "        logistic_regOp.output, gaussian_NB_Op.output, svmOp.output, decision_treesOp.output,\n",
    "        export_bucket, minio_server, minio_access_key, minio_secret_key\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def op_transformer(op):\n",
    "    op.add_pod_annotation(name=\"sidecar.istio.io/inject\", value=\"false\")\n",
    "    return op\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"Titanic Pipeline\",\n",
    "    description=\"A sample pipeline to demonstrate different model training, evaluation and export\",\n",
    ")\n",
    "def titanic_pipeline(\n",
    "    l_models_dir: str= \"/train/models\",\n",
    "    g_models_dir: str= \"/train/models\",\n",
    "    s_models_dir: str= \"/train/models\",\n",
    "    d_models_dir: str= \"/train/models\",\n",
    "    data_dir: str = \"/train/data\",\n",
    "    preprocessed_data_dir: str = \"/train/preprocessed\",\n",
    "    feature_dir: str = \"/train/features\",\n",
    "    export_bucket: str = \"titanic\"\n",
    "   \n",
    "):\n",
    "    MINIO_SERVER='minio-service.kubeflow:9000'\n",
    "    MINIO_ACCESS_KEY='minio'\n",
    "    MINIO_SECRET_KEY='minio123'\n",
    "    \n",
    "    train_model_pipeline(\n",
    "        data_dir=data_dir,\n",
    "        preprocessed_data_dir=preprocessed_data_dir,\n",
    "        feature_dir=feature_dir,\n",
    "        l_models_dir=l_models_dir,\n",
    "        g_models_dir=g_models_dir,\n",
    "        s_models_dir= s_models_dir,\n",
    "        d_models_dir=d_models_dir,\n",
    "        export_bucket=export_bucket,\n",
    "        minio_server=MINIO_SERVER,\n",
    "        minio_access_key=MINIO_ACCESS_KEY,\n",
    "        minio_secret_key=MINIO_SECRET_KEY,\n",
    "    )\n",
    "    dsl.get_pipeline_conf().add_op_transformer(op_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place, let's submit the pipeline directly from our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = titanic_pipeline\n",
    "run_name = pipeline_func.__name__ + \" run\"\n",
    "experiment_name = \"End-to-End-Demo\"\n",
    "\n",
    "\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  'titanic_pipeline.yaml')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the generated yaml file to create a pipeline in Kubeflow UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now delete your bucket when you have run the pipeline successfully in the Kubeflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ./mc rb minio/titanic --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

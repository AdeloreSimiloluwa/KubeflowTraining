apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: end-to-end-mnist-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.4.0, pipelines.kubeflow.org/pipeline_compilation_time: '2021-03-22T04:04:53.582939',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "A sample pipeline to demonstrate
      multi-step model training, evaluation and export", "inputs": [{"default": "/train/model",
      "name": "model_dir", "optional": true, "type": "String"}, {"default": "/train/data",
      "name": "data_dir", "optional": true, "type": "String"}, {"default": "mnist",
      "name": "export_bucket", "optional": true, "type": "String"}, {"default": "mnist",
      "name": "model_name", "optional": true, "type": "String"}, {"default": "1",
      "name": "model_version", "optional": true, "type": "Integer"}], "name": "End-to-End
      MNIST Pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.4.0}
spec:
  entrypoint: end-to-end-mnist-pipeline
  templates:
  - name: download-dataset
    container:
      args: [--minio-server, 'minio-service.kubeflow:9000', --data-dir, /tmp/outputs/data_dir/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def download_dataset(minio_server, data_dir):
            """Download the MNIST data set to the KFP volume to share it among all steps"""
            import urllib.request
            import tarfile
            import os
            import subprocess

            if not os.path.exists(data_dir):
                os.makedirs(data_dir)

            #this url leads to your bucket
            url = f'http://{minio_server}/mnist/datasets.tar.gz'
            stream = urllib.request.urlopen(url)
            tar = tarfile.open(fileobj=stream, mode="r|gz")
            tar.extractall(path=data_dir)

            subprocess.call(["ls", "-lha", data_dir])

        import argparse
        _parser = argparse.ArgumentParser(prog='Download dataset', description='Download the MNIST data set to the KFP volume to share it among all steps')
        _parser.add_argument("--minio-server", dest="minio_server", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--data-dir", dest="data_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = download_dataset(**_parsed_args)
      image: mavencodev/minio:v.0.1
    outputs:
      artifacts:
      - {name: download-dataset-data_dir, path: /tmp/outputs/data_dir/data}
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
        pipelines.kubeflow.org/component_spec: '{"description": "Download the MNIST
          data set to the KFP volume to share it among all steps", "implementation":
          {"container": {"args": ["--minio-server", {"inputValue": "minio_server"},
          "--data-dir", {"outputPath": "data_dir"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef download_dataset(minio_server,
          data_dir):\n    \"\"\"Download the MNIST data set to the KFP volume to share
          it among all steps\"\"\"\n    import urllib.request\n    import tarfile\n    import
          os\n    import subprocess\n\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    #this
          url leads to your bucket\n    url = f''http://{minio_server}/mnist/datasets.tar.gz''\n    stream
          = urllib.request.urlopen(url)\n    tar = tarfile.open(fileobj=stream, mode=\"r|gz\")\n    tar.extractall(path=data_dir)\n\n    subprocess.call([\"ls\",
          \"-lha\", data_dir])\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Download
          dataset'', description=''Download the MNIST data set to the KFP volume to
          share it among all steps'')\n_parser.add_argument(\"--minio-server\", dest=\"minio_server\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--data-dir\",
          dest=\"data_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = download_dataset(**_parsed_args)\n"], "image": "mavencodev/minio:v.0.1"}},
          "inputs": [{"name": "minio_server", "type": "String"}], "name": "Download
          dataset", "outputs": [{"name": "data_dir", "type": "String"}]}'
        pipelines.kubeflow.org/component_ref: '{}'
        pipelines.kubeflow.org/arguments.parameters: '{"minio_server": "minio-service.kubeflow:9000"}'
  - name: end-to-end-mnist-pipeline
    inputs:
      parameters:
      - {name: export_bucket}
      - {name: model_name}
      - {name: model_version}
    dag:
      tasks:
      - {name: download-dataset, template: download-dataset}
      - name: evaluate-model
        template: evaluate-model
        dependencies: [download-dataset, train-model]
        arguments:
          artifacts:
          - {name: download-dataset-data_dir, from: '{{tasks.download-dataset.outputs.artifacts.download-dataset-data_dir}}'}
          - {name: train-model-model_dir, from: '{{tasks.train-model.outputs.artifacts.train-model-model_dir}}'}
      - name: export-model
        template: export-model
        dependencies: [evaluate-model, train-model]
        arguments:
          parameters:
          - {name: export_bucket, value: '{{inputs.parameters.export_bucket}}'}
          - {name: model_name, value: '{{inputs.parameters.model_name}}'}
          - {name: model_version, value: '{{inputs.parameters.model_version}}'}
          artifacts:
          - {name: evaluate-model-metrics, from: '{{tasks.evaluate-model.outputs.artifacts.evaluate-model-metrics}}'}
          - {name: train-model-model_dir, from: '{{tasks.train-model.outputs.artifacts.train-model-model_dir}}'}
      - name: train-model
        template: train-model
        dependencies: [download-dataset]
        arguments:
          artifacts:
          - {name: download-dataset-data_dir, from: '{{tasks.download-dataset.outputs.artifacts.download-dataset-data_dir}}'}
  - name: evaluate-model
    container:
      args: [--data-dir, /tmp/inputs/data_dir/data, --model-dir, /tmp/inputs/model_dir/data,
        --metrics, /tmp/outputs/metrics/data, '----output-paths', /tmp/outputs/mlpipeline_metrics/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def evaluate_model(
            data_dir, model_dir, metrics_path
        ):
            """Loads a saved model from file and uses a pre-downloaded dataset for evaluation.
            Model metrics are persisted to `/mlpipeline-metrics.json` for Kubeflow Pipelines
            metadata."""

            import json
            import tensorflow as tf
            import tensorflow_datasets as tfds
            from collections import namedtuple

            def normalize_image(image, label):
                return tf.cast(image, tf.float32) / 255.0, label

            ds_test, ds_info = tfds.load(
                "mnist",
                split="test",
                shuffle_files=True,
                as_supervised=True,
                with_info=True,
                download=True,
                data_dir=f"{data_dir}/datasets",
            )

            ds_test = ds_test.map(
                normalize_image, num_parallel_calls=tf.data.experimental.AUTOTUNE
            )
            ds_test = ds_test.batch(128)
            ds_test = ds_test.cache()
            ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)

            model = tf.keras.models.load_model(model_dir)
            (loss, accuracy) = model.evaluate(ds_test)

            metrics = {
                "metrics": [
                    {"name": "loss", "numberValue": str(loss), "format": "PERCENTAGE"},
                    {"name": "accuracy", "numberValue": str(accuracy), "format": "PERCENTAGE"},
                ]
            }

            with open(metrics_path, "w") as f:
                json.dump(metrics, f)

            out_tuple = namedtuple("EvaluationOutput", ["mlpipeline_metrics"])

            return out_tuple(json.dumps(metrics))

        import argparse
        _parser = argparse.ArgumentParser(prog='Evaluate model', description='Loads a saved model from file and uses a pre-downloaded dataset for evaluation.')
        _parser.add_argument("--data-dir", dest="data_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model-dir", dest="model_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--metrics", dest="metrics_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = evaluate_model(**_parsed_args)

        _output_serializers = [
            str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: mavencodev/minio:v.0.1
    inputs:
      artifacts:
      - {name: download-dataset-data_dir, path: /tmp/inputs/data_dir/data}
      - {name: train-model-model_dir, path: /tmp/inputs/model_dir/data}
    outputs:
      artifacts:
      - {name: mlpipeline-metrics, path: /tmp/outputs/mlpipeline_metrics/data}
      - {name: evaluate-model-metrics, path: /tmp/outputs/metrics/data}
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
        pipelines.kubeflow.org/component_spec: '{"description": "Loads a saved model
          from file and uses a pre-downloaded dataset for evaluation.", "implementation":
          {"container": {"args": ["--data-dir", {"inputPath": "data_dir"}, "--model-dir",
          {"inputPath": "model_dir"}, "--metrics", {"outputPath": "metrics"}, "----output-paths",
          {"outputPath": "mlpipeline_metrics"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef evaluate_model(\n    data_dir,
          model_dir, metrics_path\n):\n    \"\"\"Loads a saved model from file and
          uses a pre-downloaded dataset for evaluation.\n    Model metrics are persisted
          to `/mlpipeline-metrics.json` for Kubeflow Pipelines\n    metadata.\"\"\"\n\n    import
          json\n    import tensorflow as tf\n    import tensorflow_datasets as tfds\n    from
          collections import namedtuple\n\n    def normalize_image(image, label):\n        return
          tf.cast(image, tf.float32) / 255.0, label\n\n    ds_test, ds_info = tfds.load(\n        \"mnist\",\n        split=\"test\",\n        shuffle_files=True,\n        as_supervised=True,\n        with_info=True,\n        download=True,\n        data_dir=f\"{data_dir}/datasets\",\n    )\n\n    ds_test
          = ds_test.map(\n        normalize_image, num_parallel_calls=tf.data.experimental.AUTOTUNE\n    )\n    ds_test
          = ds_test.batch(128)\n    ds_test = ds_test.cache()\n    ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n\n    model
          = tf.keras.models.load_model(model_dir)\n    (loss, accuracy) = model.evaluate(ds_test)\n\n    metrics
          = {\n        \"metrics\": [\n            {\"name\": \"loss\", \"numberValue\":
          str(loss), \"format\": \"PERCENTAGE\"},\n            {\"name\": \"accuracy\",
          \"numberValue\": str(accuracy), \"format\": \"PERCENTAGE\"},\n        ]\n    }\n\n    with
          open(metrics_path, \"w\") as f:\n        json.dump(metrics, f)\n\n    out_tuple
          = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n\n    return
          out_tuple(json.dumps(metrics))\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Evaluate
          model'', description=''Loads a saved model from file and uses a pre-downloaded
          dataset for evaluation.'')\n_parser.add_argument(\"--data-dir\", dest=\"data_dir\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-dir\",
          dest=\"model_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--metrics\",
          dest=\"metrics_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\",
          type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = evaluate_model(**_parsed_args)\n\n_output_serializers
          = [\n    str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "mavencodev/minio:v.0.1"}}, "inputs": [{"name": "data_dir", "type":
          "String"}, {"name": "model_dir", "type": "String"}], "name": "Evaluate model",
          "outputs": [{"name": "metrics", "type": "String"}, {"name": "mlpipeline_metrics",
          "type": "Metrics"}]}'
        pipelines.kubeflow.org/component_ref: '{}'
  - name: export-model
    container:
      args: [--model-dir, /tmp/inputs/model_dir/data, --metrics, /tmp/inputs/metrics/data,
        --export-bucket, '{{inputs.parameters.export_bucket}}', --model-name, '{{inputs.parameters.model_name}}',
        --model-version, '{{inputs.parameters.model_version}}', --minio-server, 'minio-service.kubeflow:9000',
        --minio-access-key, minio, --minio-secret-key, minio123]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def export_model(
            model_dir,
            metrics,
            export_bucket,
            model_name,
            model_version,
            minio_server,
            minio_access_key,
            minio_secret_key,
        ):
            import os
            import boto3
            from botocore.client import Config

            s3 = boto3.client(
                "s3",
                endpoint_url=f'http://{minio_server}',
                aws_access_key_id=minio_access_key,
                aws_secret_access_key=minio_secret_key,
                config=Config(signature_version="s3v4"),
            )

            # Create export bucket if it does not yet exist
            response = s3.list_buckets()
            export_bucket_exists = False

            print(response , export_bucket)
            for bucket in response["Buckets"]:
                if bucket["Name"] == export_bucket:
                    export_bucket_exists = True

            if not export_bucket_exists:
                s3.create_bucket(ACL="public-read-write", Bucket=export_bucket)

            # Save model files to S3
            for root, dirs, files in os.walk(model_dir):
                for filename in files:
                    local_path = os.path.join(root, filename)
                    s3_path = os.path.relpath(local_path, model_dir)

                    s3.upload_file(
                        local_path,
                        export_bucket,
                        f"{model_name}/{model_version}/{s3_path}",
                        ExtraArgs={"ACL": "public-read"},
                    )

            response = s3.list_objects(Bucket=export_bucket)
            print(f"All objects in {export_bucket}:")
            for file in response["Contents"]:
                print("{}/{}".format(export_bucket, file["Key"]))

        import argparse
        _parser = argparse.ArgumentParser(prog='Export model', description='')
        _parser.add_argument("--model-dir", dest="model_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--metrics", dest="metrics", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--export-bucket", dest="export_bucket", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model-name", dest="model_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model-version", dest="model_version", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--minio-server", dest="minio_server", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--minio-access-key", dest="minio_access_key", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--minio-secret-key", dest="minio_secret_key", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = export_model(**_parsed_args)
      image: mavencodev/minio:v.0.1
    inputs:
      parameters:
      - {name: export_bucket}
      - {name: model_name}
      - {name: model_version}
      artifacts:
      - {name: evaluate-model-metrics, path: /tmp/inputs/metrics/data}
      - {name: train-model-model_dir, path: /tmp/inputs/model_dir/data}
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--model-dir", {"inputPath": "model_dir"}, "--metrics", {"inputPath": "metrics"},
          "--export-bucket", {"inputValue": "export_bucket"}, "--model-name", {"inputValue":
          "model_name"}, "--model-version", {"inputValue": "model_version"}, "--minio-server",
          {"inputValue": "minio_server"}, "--minio-access-key", {"inputValue": "minio_access_key"},
          "--minio-secret-key", {"inputValue": "minio_secret_key"}], "command": ["sh",
          "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def export_model(\n    model_dir,\n    metrics,\n    export_bucket,\n    model_name,\n    model_version,\n    minio_server,\n    minio_access_key,\n    minio_secret_key,\n):\n    import
          os\n    import boto3\n    from botocore.client import Config\n\n    s3 =
          boto3.client(\n        \"s3\",\n        endpoint_url=f''http://{minio_server}'',\n        aws_access_key_id=minio_access_key,\n        aws_secret_access_key=minio_secret_key,\n        config=Config(signature_version=\"s3v4\"),\n    )\n\n    #
          Create export bucket if it does not yet exist\n    response = s3.list_buckets()\n    export_bucket_exists
          = False\n\n    print(response , export_bucket)\n    for bucket in response[\"Buckets\"]:\n        if
          bucket[\"Name\"] == export_bucket:\n            export_bucket_exists = True\n\n    if
          not export_bucket_exists:\n        s3.create_bucket(ACL=\"public-read-write\",
          Bucket=export_bucket)\n\n    # Save model files to S3\n    for root, dirs,
          files in os.walk(model_dir):\n        for filename in files:\n            local_path
          = os.path.join(root, filename)\n            s3_path = os.path.relpath(local_path,
          model_dir)\n\n            s3.upload_file(\n                local_path,\n                export_bucket,\n                f\"{model_name}/{model_version}/{s3_path}\",\n                ExtraArgs={\"ACL\":
          \"public-read\"},\n            )\n\n    response = s3.list_objects(Bucket=export_bucket)\n    print(f\"All
          objects in {export_bucket}:\")\n    for file in response[\"Contents\"]:\n        print(\"{}/{}\".format(export_bucket,
          file[\"Key\"]))\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Export
          model'', description='''')\n_parser.add_argument(\"--model-dir\", dest=\"model_dir\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--metrics\",
          dest=\"metrics\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--export-bucket\",
          dest=\"export_bucket\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-name\",
          dest=\"model_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-version\",
          dest=\"model_version\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-server\",
          dest=\"minio_server\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-access-key\",
          dest=\"minio_access_key\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--minio-secret-key\",
          dest=\"minio_secret_key\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = export_model(**_parsed_args)\n"],
          "image": "mavencodev/minio:v.0.1"}}, "inputs": [{"name": "model_dir", "type":
          "String"}, {"name": "metrics", "type": "String"}, {"name": "export_bucket",
          "type": "String"}, {"name": "model_name", "type": "String"}, {"name": "model_version",
          "type": "Integer"}, {"name": "minio_server", "type": "String"}, {"name":
          "minio_access_key", "type": "String"}, {"name": "minio_secret_key", "type":
          "String"}], "name": "Export model"}'
        pipelines.kubeflow.org/component_ref: '{}'
        pipelines.kubeflow.org/arguments.parameters: '{"export_bucket": "{{inputs.parameters.export_bucket}}",
          "minio_access_key": "minio", "minio_secret_key": "minio123", "minio_server":
          "minio-service.kubeflow:9000", "model_name": "{{inputs.parameters.model_name}}",
          "model_version": "{{inputs.parameters.model_version}}"}'
  - name: train-model
    container:
      args: [--data-dir, /tmp/inputs/data_dir/data, --model-dir, /tmp/outputs/model_dir/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def train_model(data_dir, model_dir):
            """Trains a single-layer CNN for 5 epochs using a pre-downloaded dataset.
            Once trained, the model is persisted to `model_dir`."""

            import os
            import tensorflow as tf
            import tensorflow_datasets as tfds

            def normalize_image(image, label):
                """Normalizes images: `uint8` -> `float32`"""
                return tf.cast(image, tf.float32) / 255.0, label

            model = tf.keras.models.Sequential(
                [
                    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),
                    tf.keras.layers.Dense(128, activation="relu"),
                    tf.keras.layers.Dense(10, activation="softmax"),
                ]
            )
            model.compile(
                loss="sparse_categorical_crossentropy",
                optimizer=tf.keras.optimizers.Adam(0.001),
                metrics=["accuracy"],
            )

            print(model.summary())
            ds_train, ds_info = tfds.load(
                "mnist",
                split="train",
                shuffle_files=True,
                as_supervised=True,
                with_info=True,
                download=True,
                data_dir=f"{data_dir}/datasets",
            )

            ds_train = ds_train.map(
                normalize_image, num_parallel_calls=tf.data.experimental.AUTOTUNE
            )
            ds_train = ds_train.cache()
            ds_train = ds_train.shuffle(ds_info.splits["train"].num_examples)
            ds_train = ds_train.batch(128)
            ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)

            model.fit(
                ds_train,
                epochs=5,
            )

            model.save(model_dir)
            print(f"Model saved {model_dir}")
            print(os.listdir(model_dir))

        import argparse
        _parser = argparse.ArgumentParser(prog='Train model', description='Trains a single-layer CNN for 5 epochs using a pre-downloaded dataset.')
        _parser.add_argument("--data-dir", dest="data_dir", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model-dir", dest="model_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = train_model(**_parsed_args)
      image: mavencodev/minio:v.0.1
    inputs:
      artifacts:
      - {name: download-dataset-data_dir, path: /tmp/inputs/data_dir/data}
    outputs:
      artifacts:
      - {name: train-model-model_dir, path: /tmp/outputs/model_dir/data}
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
        pipelines.kubeflow.org/component_spec: '{"description": "Trains a single-layer
          CNN for 5 epochs using a pre-downloaded dataset.", "implementation": {"container":
          {"args": ["--data-dir", {"inputPath": "data_dir"}, "--model-dir", {"outputPath":
          "model_dir"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef train_model(data_dir, model_dir):\n    \"\"\"Trains
          a single-layer CNN for 5 epochs using a pre-downloaded dataset.\n    Once
          trained, the model is persisted to `model_dir`.\"\"\"\n\n    import os\n    import
          tensorflow as tf\n    import tensorflow_datasets as tfds\n\n    def normalize_image(image,
          label):\n        \"\"\"Normalizes images: `uint8` -> `float32`\"\"\"\n        return
          tf.cast(image, tf.float32) / 255.0, label\n\n    model = tf.keras.models.Sequential(\n        [\n            tf.keras.layers.Flatten(input_shape=(28,
          28, 1)),\n            tf.keras.layers.Dense(128, activation=\"relu\"),\n            tf.keras.layers.Dense(10,
          activation=\"softmax\"),\n        ]\n    )\n    model.compile(\n        loss=\"sparse_categorical_crossentropy\",\n        optimizer=tf.keras.optimizers.Adam(0.001),\n        metrics=[\"accuracy\"],\n    )\n\n    print(model.summary())\n    ds_train,
          ds_info = tfds.load(\n        \"mnist\",\n        split=\"train\",\n        shuffle_files=True,\n        as_supervised=True,\n        with_info=True,\n        download=True,\n        data_dir=f\"{data_dir}/datasets\",\n    )\n\n    ds_train
          = ds_train.map(\n        normalize_image, num_parallel_calls=tf.data.experimental.AUTOTUNE\n    )\n    ds_train
          = ds_train.cache()\n    ds_train = ds_train.shuffle(ds_info.splits[\"train\"].num_examples)\n    ds_train
          = ds_train.batch(128)\n    ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n\n    model.fit(\n        ds_train,\n        epochs=5,\n    )\n\n    model.save(model_dir)\n    print(f\"Model
          saved {model_dir}\")\n    print(os.listdir(model_dir))\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Train model'', description=''Trains a single-layer
          CNN for 5 epochs using a pre-downloaded dataset.'')\n_parser.add_argument(\"--data-dir\",
          dest=\"data_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-dir\",
          dest=\"model_dir\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_model(**_parsed_args)\n"], "image": "mavencodev/minio:v.0.1"}},
          "inputs": [{"name": "data_dir", "type": "String"}], "name": "Train model",
          "outputs": [{"name": "model_dir", "type": "String"}]}'
        pipelines.kubeflow.org/component_ref: '{}'
  arguments:
    parameters:
    - {name: model_dir, value: /train/model}
    - {name: data_dir, value: /train/data}
    - {name: export_bucket, value: mnist}
    - {name: model_name, value: mnist}
    - {name: model_version, value: '1'}
  serviceAccountName: pipeline-runner

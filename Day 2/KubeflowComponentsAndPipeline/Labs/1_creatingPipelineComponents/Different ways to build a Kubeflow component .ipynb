{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install and import pip and kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/jovyan/.local/lib/python3.6/site-packages (21.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --user --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp in /home/jovyan/.local/lib/python3.6/site-packages (1.4.0)\n",
      "Requirement already satisfied: fire>=0.3.1 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (0.4.0)\n",
      "Requirement already satisfied: docstring-parser>=0.7.3 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (0.7.3)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.25.0)\n",
      "Requirement already satisfied: strip-hints in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (1.3.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from kfp) (1.2.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.6/dist-packages (from kfp) (5.3)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (10.0.1)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.11.0)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: tabulate in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (0.8.9)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.0 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (0.1.6)\n",
      "Requirement already satisfied: click in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied: Deprecated in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (1.2.11)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.3.1->kfp) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire>=0.3.1->kfp) (1.11.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (45.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (0.5.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2019.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.51.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /home/jovyan/.local/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.15.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (19.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (0.15.7)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (1.4.0)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.25.8)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2019.11.28)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.0.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from Deprecated->kfp) (1.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonschema>=3.0.1->kfp) (2.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.1.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints->kfp) (0.30.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install kfp --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please restart your notebook before you proceed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a lightweight component from a python function\n",
    "\n",
    "       a.  Define the python function with all its dependencies installed and imported within it\n",
    "       b.  Download your data\n",
    "       c.  Write your function and return its output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_path,train_data,test_data):\n",
    "    # install libraries \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn==0.22'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas==0.23.4'])\n",
    "    # import libraries\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "    #download the data\n",
    "    data = pd.read_csv(\"https://raw.githubusercontent.com/MavenCode/KubeflowTraining/master/Data/Churn_Modelling.csv\")\n",
    "\n",
    "    #dropping some columns that are not needed\n",
    "    data = data.drop(columns=['RowNumber','CustomerId','Surname'], axis=1)\n",
    "    #data features\n",
    "    X = data.iloc[:,:-1]\n",
    "    #target data\n",
    "    y = data.iloc[:,-1:]   \n",
    "    #encoding the categorical columns\n",
    "    le = LabelEncoder()\n",
    "    ohe = OneHotEncoder()\n",
    "    X['Gender'] = le.fit_transform(X['Gender'])\n",
    "    geo_df = pd.DataFrame(ohe.fit_transform(X[['Geography']]).toarray())\n",
    "\n",
    "    #getting feature name after onehotencoding\n",
    "    geo_df.columns = ohe.get_feature_names(['Geography'])\n",
    "\n",
    "    #merging geo_df with the main data\n",
    "    X = X.join(geo_df) \n",
    "    #dropping the old columns after encoding\n",
    "    X.drop(columns=['Geography'], axis=1, inplace=True)\n",
    "\n",
    "    #splitting the data \n",
    "    X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.2, random_state = 42)\n",
    "    #feature scaling\n",
    "    sc =StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    #saving the values from the dataframe\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "    \n",
    "    #Save the train_data as a pickle file to be used by the train component.\n",
    "    with open(f'{data_path}/{train_data}', 'wb') as f:\n",
    "        pickle.dump((X_train,  y_train), f)\n",
    "        \n",
    "    #Save the test_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/{test_data}', 'wb') as f:\n",
    "        pickle.dump((X_test,  y_test), f)\n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert your funtion into a lightweight component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_op = kfp.components.create_component_from_func(preprocess,base_image=\"python:3.7.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the preprocess_op as a kubeflow pipeline component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Buiding a Reusable component from a python script\n",
    "\n",
    "    \n",
    "    a. Write your python function in a .py script\n",
    "    b. Build and push your docker image\n",
    "    c. Build a resuable component with your image and script\n",
    "    \n",
    "**NOTE: The python script and docker image below, are not executed in this notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Python script named \"reusable_preprocess.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "def preprocess():\n",
    "    #importing libraries\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "    \n",
    "    #importing the data\n",
    "    data = pd.read_csv(\"https://raw.githubusercontent.com/MavenCode/KubeflowTraining/master/Data/Churn_Modelling.csv\")\n",
    "    #dropping some columns that are not needed\n",
    "    data = data.drop(columns=['RowNumber','CustomerId','Surname'], axis=1)\n",
    "    #data features\n",
    "    X = data.iloc[:,:-1]\n",
    "    #target data\n",
    "    y = data.iloc[:,-1:]   \n",
    "    #encoding the categorical columns\n",
    "    le = LabelEncoder()\n",
    "    ohe = OneHotEncoder()\n",
    "    X['Gender'] = le.fit_transform(X['Gender'])\n",
    "    geo_df = pd.DataFrame(ohe.fit_transform(X[['Geography']]).toarray())\n",
    "\n",
    "    #getting feature name after onehotencoding\n",
    "    geo_df.columns = ohe.get_feature_names(['Geography'])\n",
    "\n",
    "    #merging geo_df with the main data\n",
    "    X = X.join(geo_df) \n",
    "    #dropping the old columns after encoding\n",
    "    X.drop(columns=['Geography'], axis=1, inplace=True)\n",
    "\n",
    "    #splitting the data \n",
    "    X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.2, random_state = 42)\n",
    "    #feature scaling\n",
    "    sc =StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    #saving output file to path\n",
    "    np.save('X_train.npy', X_train)\n",
    "    np.save('X_test.npy', X_test)\n",
    "    np.save('y_train.npy', y_train)\n",
    "    np.save('y_test.npy', y_test)\n",
    "\n",
    "#defining and parsing arguments\n",
    "if __name__ == '__main__':\n",
    "    preprocess()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Dockerfile\n",
    "**This Docker image was pushed to dockerhub with the steps detailed in the slides**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM python:3.7.1\n",
    "WORKDIR /preprocess_data\n",
    "RUN pip install --upgrade pip \\\n",
    "&& pip install -U scikit-learn numpy pandas\n",
    "COPY preprocess.py /preprocess_data\n",
    "ENTRYPOINT [\"python\", \"preprocess.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define your python function as a kubeflow component  using the Kubeflow Pipelines DSL . The DSL defines your pipeline’s interactions with the component’s Docker container.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_op(data):\n",
    "    return dsl.ContainerOp(\n",
    "        name = 'Preprocess Data',\n",
    "        # docker image\n",
    "        image = 'mavencodev/preprocess-component:v.0.2',\n",
    "        arguments = [],\n",
    "        # component outputs\n",
    "        file_outputs={\n",
    "            'X_train': '/preprocess_data/X_train.npy',\n",
    "            'X_test': '/preprocess_data/X_test.npy',\n",
    "            'y_train': '/preprocess_data/y_train.npy',\n",
    "            'y_test': '/preprocess_data/y_test.npy'     \n",
    "        }\n",
    "    )\n",
    "# exporting the component as a yaml file\n",
    "if __name__ == \"__main__\":\n",
    "    kfp.components.create_component_from_func(\n",
    "    preprocess_op, #function name\n",
    "    output_component_file=\"preprocess-reusable.yaml\") ,\n",
    "    base_image=\"python:3.7.1\",\n",
    "    packages_to_install = [\"pandas==0.23.4\", \"scikit-learn==0.22\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  Create a yaml file from a lightweight component\n",
    "\n",
    "You can also create a yaml file with a lightweight component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_path,train_data,test_data):\n",
    "    import pickle\n",
    "    # import Library\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn==0.22'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas==0.23.4'])\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "    #importing the data\n",
    "    data = pd.read_csv(\"https://raw.githubusercontent.com/MavenCode/KubeflowTraining/master/Data/Churn_Modelling.csv\")\n",
    "\n",
    "    #dropping some columns that are not needed\n",
    "    data = data.drop(columns=['RowNumber','CustomerId','Surname'], axis=1)\n",
    "    #data features\n",
    "    X = data.iloc[:,:-1]\n",
    "    #target data\n",
    "    y = data.iloc[:,-1:]   \n",
    "    #encoding the categorical columns\n",
    "    le = LabelEncoder()\n",
    "    ohe = OneHotEncoder()\n",
    "    X['Gender'] = le.fit_transform(X['Gender'])\n",
    "    geo_df = pd.DataFrame(ohe.fit_transform(X[['Geography']]).toarray())\n",
    "\n",
    "    #getting feature name after onehotencoding\n",
    "    geo_df.columns = ohe.get_feature_names(['Geography'])\n",
    "\n",
    "    #merging geo_df with the main data\n",
    "    X = X.join(geo_df) \n",
    "    #dropping the old columns after encoding\n",
    "    X.drop(columns=['Geography'], axis=1, inplace=True)\n",
    "\n",
    "    #splitting the data \n",
    "    X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.2, random_state = 42)\n",
    "    #feature scaling\n",
    "    sc =StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    #saving the values from the dataframe\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "    \n",
    "    #Save the train_data as a pickle file to be used by the train component.\n",
    "    with open(f'{data_path}/{train_data}', 'wb') as f:\n",
    "        pickle.dump((X_train,  y_train), f)\n",
    "        \n",
    "    #Save the test_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/{test_data}', 'wb') as f:\n",
    "        pickle.dump((X_test,  y_test), f)\n",
    "    \n",
    "    return(print('Done!'))\n",
    "\n",
    "#exporting function to YAML file\n",
    "if __name__ == \"__main__\":\n",
    "    kfp.components.create_component_from_func(\n",
    "    preprocess, #function name\n",
    "    output_component_file=\"preprocess-component.yaml\") ,\n",
    "    base_image=\"python:3.7.1\",\n",
    "    packages_to_install = [\"pandas==0.23.4\", \"scikit-learn==0.22\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The YAML file is created in the working directory of the notebook except if it is specified otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is what the yaml file looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name: Preprocess\n",
    "inputs:\n",
    "- {name: data_path}\n",
    "- {name: train_data}\n",
    "- {name: test_data}\n",
    "implementation:\n",
    "  container:\n",
    "    image: python:3.7\n",
    "    command:\n",
    "    - sh\n",
    "    - -ec\n",
    "    - |\n",
    "      program_path=$(mktemp)\n",
    "      printf \"%s\" \"$0\" > \"$program_path\"\n",
    "      python3 -u \"$program_path\" \"$@\"\n",
    "    - \"def preprocess(data_path,train_data,test_data):\\n    import pickle\\n    # import\\\n",
    "      \\ Library\\n    import sys, subprocess;\\n    subprocess.run([sys.executable,\\\n",
    "      \\ '-m', 'pip', 'install','scikit-learn==0.22'])\\n    subprocess.run([sys.executable,\\\n",
    "      \\ '-m', 'pip', 'install','pandas==0.23.4'])\\n    import pandas as pd\\n    import\\\n",
    "      \\ numpy as np\\n    from sklearn.preprocessing import LabelEncoder\\n    from\\\n",
    "      \\ sklearn.preprocessing import OneHotEncoder\\n    from sklearn.model_selection\\\n",
    "      \\ import train_test_split\\n    from sklearn.preprocessing import StandardScaler\\\n",
    "      \\ \\n\\n    #importing the data\\n    data = pd.read_csv(\\\"https://raw.githubusercontent.com/MavenCode/KubeflowTraining/master/Data/Churn_Modelling.csv\\\"\\\n",
    "      )\\n\\n    #dropping some columns that are not needed\\n    data = data.drop(columns=['RowNumber','CustomerId','Surname'],\\\n",
    "      \\ axis=1)\\n    #data features\\n    X = data.iloc[:,:-1]\\n    #target data\\n\\\n",
    "      \\    y = data.iloc[:,-1:]   \\n    #encoding the categorical columns\\n    le\\\n",
    "      \\ = LabelEncoder()\\n    ohe = OneHotEncoder()\\n    X['Gender'] = le.fit_transform(X['Gender'])\\n\\\n",
    "      \\    geo_df = pd.DataFrame(ohe.fit_transform(X[['Geography']]).toarray())\\n\\n\\\n",
    "      \\    #getting feature name after onehotencoding\\n    geo_df.columns = ohe.get_feature_names(['Geography'])\\n\\\n",
    "      \\n    #merging geo_df with the main data\\n    X = X.join(geo_df) \\n    #dropping\\\n",
    "      \\ the old columns after encoding\\n    X.drop(columns=['Geography'], axis=1,\\\n",
    "      \\ inplace=True)\\n\\n    #splitting the data \\n    X_train,X_test,y_train,y_test\\\n",
    "      \\ = train_test_split( X,y, test_size=0.2, random_state = 42)\\n    #feature scaling\\n\\\n",
    "      \\    sc =StandardScaler()\\n    X_train = sc.fit_transform(X_train)\\n    X_test\\\n",
    "      \\ = sc.transform(X_test)\\n    #saving the values from the dataframe\\n    y_train\\\n",
    "      \\ = y_train.values\\n    y_test = y_test.values\\n\\n    #Save the train_data as\\\n",
    "      \\ a pickle file to be used by the train component.\\n    with open(f'{data_path}/{train_data}',\\\n",
    "      \\ 'wb') as f:\\n        pickle.dump((X_train,  y_train), f)\\n\\n    #Save the\\\n",
    "      \\ test_data as a pickle file to be used by the predict component.\\n    with\\\n",
    "      \\ open(f'{data_path}/{test_data}', 'wb') as f:\\n        pickle.dump((X_test,\\\n",
    "      \\  y_test), f)\\n\\n    return(print('Done!'))\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog='Preprocess',\\\n",
    "      \\ description='')\\n_parser.add_argument(\\\"--data-path\\\", dest=\\\"data_path\\\"\\\n",
    "      , type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"\\\n",
    "      --train-data\\\", dest=\\\"train_data\\\", type=str, required=True, default=argparse.SUPPRESS)\\n\\\n",
    "      _parser.add_argument(\\\"--test-data\\\", dest=\\\"test_data\\\", type=str, required=True,\\\n",
    "      \\ default=argparse.SUPPRESS)\\n_parsed_args = vars(_parser.parse_args())\\n\\n\\\n",
    "      _outputs = preprocess(**_parsed_args)\\n\"\n",
    "    args:\n",
    "    - --data-path\n",
    "    - {inputValue: data_path}\n",
    "    - --train-data\n",
    "    - {inputValue: train_data}\n",
    "    - --test-data\n",
    "    - {inputValue: test_data}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### kfp.components.load_component_from_file or kfp.components.load_component_from_url  can be used to load the yaml file in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Preprocess(data_path, train_data, test_data)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfp.components.load_component_from_file(\"preprocess-component.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The yaml life above can be defined even better by calling the python script this way:\n",
    "\n",
    "This can only be built manually.\n",
    "\n",
    "**PS: DO NOT RUN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name your component\n",
    "name:  preprocessing\n",
    "description:  transforms the data into the best form for training.\n",
    "#define your inputs, outputs and their types\n",
    "inputs:\n",
    "- {name: data_path,  type: }\n",
    "\n",
    "outputs:\n",
    "- {name: output_train_data, type: }\n",
    "- {name: output_test_data, type: }\n",
    "\n",
    "implementation:\n",
    "  container:\n",
    "    # docker image\n",
    "    image: mavencodev/preprocess-component:v.0.2\n",
    "    command: [\n",
    "      # python script   \n",
    "      python3, /KubeflowTraining/Day 2/KubeflowComponentsAndPipeline/Labs/1_creatingPipelineComponents/reusable_componet.py,\n",
    "      # define your outputs\n",
    "      --input_path,             {inputValue: input_data_path},\n",
    "      --output_dir,             {inputValue: output_train_data},\n",
    "      --output_dir,             {inputValue: output_test_data}\n",
    "      \n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yaml would be pushed to you github repository and called in when compiling your pipeline using `kfp.components.load_component_from_file(\"https://raw.githubusercontent.com/MavenCode....yaml\")`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

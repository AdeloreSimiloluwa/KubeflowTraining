{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.local/lib/python3.6/site-packages (21.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "#!python -m pip install --user --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: kfp in ./.local/lib/python3.6/site-packages (1.3.0)\n",
      "Collecting kfp\n",
      "  Downloading kfp-1.4.0.tar.gz (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.6/dist-packages (from kfp) (5.3)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.25.0)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (10.0.1)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.10.0)\n",
      "Requirement already satisfied: requests_toolbelt>=0.8.0 in ./.local/lib/python3.6/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from kfp) (1.2.2)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in ./.local/lib/python3.6/site-packages (from kfp) (1.3.0)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: tabulate in ./.local/lib/python3.6/site-packages (from kfp) (0.8.7)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.6/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied: Deprecated in ./.local/lib/python3.6/site-packages (from kfp) (1.2.11)\n",
      "Requirement already satisfied: strip-hints in ./.local/lib/python3.6/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied: docstring-parser>=0.7.3 in ./.local/lib/python3.6/site-packages (from kfp) (0.7.3)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.0 in ./.local/lib/python3.6/site-packages (from kfp) (0.1.4)\n",
      "Collecting fire>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 6.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.3.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.3.1->kfp) (1.1.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (44.0.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (0.5.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2019.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.51.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.11.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (19.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (1.4.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (0.15.7)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.25.7)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2019.11.28)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.6)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from Deprecated->kfp) (1.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonschema>=3.0.1->kfp) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata->jsonschema>=3.0.1->kfp) (8.0.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.1.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints->kfp) (0.30.0)\n",
      "Building wheels for collected packages: kfp, fire\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-1.4.0-py3-none-any.whl size=222156 sha256=de633a2d7bba9725d1d6ef5c0215a21cd9380aa6313e9c3e8b33a140277a379d\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/88/63/63/f727a62aaba1e0fe13fe549e1b7538e9b8a2bc43dcae8138c8\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=117174 sha256=21db6e5c95672c82b6e50037dbc89c4560a5d80a67968e6aa708cfbf7696b131\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/a6/12/74/ce0728e3990845862240349a12d7179a262e388ec73938024b\n",
      "Successfully built kfp fire\n",
      "Installing collected packages: fire, kfp\n",
      "  Attempting uninstall: kfp\n",
      "    Found existing installation: kfp 1.3.0\n",
      "    Uninstalling kfp-1.3.0:\n",
      "      Successfully uninstalled kfp-1.3.0\n",
      "\u001b[33m  WARNING: The scripts dsl-compile, dsl-compile-v2 and kfp are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed fire-0.4.0 kfp-1.4.0\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install kfp --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using python function to create lightweight component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the python function with all its dependencies installed and imported within it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_path,train_data,test_data):\n",
    "    import pickle\n",
    "    # import Library\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn==0.22'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas==0.23.4'])\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "    #importing the data\n",
    "    data = pd.read_csv(\"https://raw.githubusercontent.com/MavenCode/KubeflowTraining/master/Data/Churn_Modelling.csv\")\n",
    "\n",
    "    #dropping some columns that are not needed\n",
    "    data = data.drop(columns=['RowNumber','CustomerId','Surname'], axis=1)\n",
    "    #data features\n",
    "    X = data.iloc[:,:-1]\n",
    "    #target data\n",
    "    y = data.iloc[:,-1:]   \n",
    "    #encoding the categorical columns\n",
    "    le = LabelEncoder()\n",
    "    ohe = OneHotEncoder()\n",
    "    X['Gender'] = le.fit_transform(X['Gender'])\n",
    "    geo_df = pd.DataFrame(ohe.fit_transform(X[['Geography']]).toarray())\n",
    "\n",
    "    #getting feature name after onehotencoding\n",
    "    geo_df.columns = ohe.get_feature_names(['Geography'])\n",
    "\n",
    "    #merging geo_df with the main data\n",
    "    X = X.join(geo_df) \n",
    "    #dropping the old columns after encoding\n",
    "    X.drop(columns=['Geography'], axis=1, inplace=True)\n",
    "\n",
    "    #splitting the data \n",
    "    X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.2, random_state = 42)\n",
    "    #feature scaling\n",
    "    sc =StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    #saving the values from the dataframe\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "    \n",
    "    #Save the train_data as a pickle file to be used by the train component.\n",
    "    with open(f'{data_path}/{train_data}', 'wb') as f:\n",
    "        pickle.dump((X_train,  y_train), f)\n",
    "        \n",
    "    #Save the test_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/{test_data}', 'wb') as f:\n",
    "        pickle.dump((X_test,  y_test), f)\n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting the python function to a component "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_op = kfp.components.create_component_from_func(preprocess,base_image=\"python:3.7.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the preprocess_op as a kubeflow pipeline component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using python function to create reusable component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the python script and docker image below, are not executed on this notebook but using your desired choice of text editor. VSCode was used to execute these ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### python script. Named \"preprocess.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "def preprocess():\n",
    "    #importing libraries\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "    \n",
    "    #importing the data\n",
    "    data = pd.read_csv(\"https://raw.githubusercontent.com/MavenCode/KubeflowTraining/master/Data/Churn_Modelling.csv\")\n",
    "    #dropping some columns that are not needed\n",
    "    data = data.drop(columns=['RowNumber','CustomerId','Surname'], axis=1)\n",
    "    #data features\n",
    "    X = data.iloc[:,:-1]\n",
    "    #target data\n",
    "    y = data.iloc[:,-1:]   \n",
    "    #encoding the categorical columns\n",
    "    le = LabelEncoder()\n",
    "    ohe = OneHotEncoder()\n",
    "    X['Gender'] = le.fit_transform(X['Gender'])\n",
    "    geo_df = pd.DataFrame(ohe.fit_transform(X[['Geography']]).toarray())\n",
    "\n",
    "    #getting feature name after onehotencoding\n",
    "    geo_df.columns = ohe.get_feature_names(['Geography'])\n",
    "\n",
    "    #merging geo_df with the main data\n",
    "    X = X.join(geo_df) \n",
    "    #dropping the old columns after encoding\n",
    "    X.drop(columns=['Geography'], axis=1, inplace=True)\n",
    "\n",
    "    #splitting the data \n",
    "    X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.2, random_state = 42)\n",
    "    #feature scaling\n",
    "    sc =StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    #saving output file to path\n",
    "    np.save('X_train.npy', X_train)\n",
    "    np.save('X_test.npy', X_test)\n",
    "    np.save('y_train.npy', y_train)\n",
    "    np.save('y_test.npy', y_test)\n",
    "\n",
    "#defining and parsing arguments\n",
    "if __name__ == '__main__':\n",
    "    preprocess()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dockerfile\n",
    "#### This Docker image was pushed to dockerhub. Steps to push to docker hub was explained in the slides and labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM python:3.7.1\n",
    "WORKDIR /preprocess_data\n",
    "RUN pip install --upgrade pip \\\n",
    "&& pip install -U scikit-learn numpy pandas\n",
    "COPY preprocess.py /preprocess_data\n",
    "ENTRYPOINT [\"python\", \"preprocess.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### writing the component python function using the Kubeflow Pipelines DSL to define your pipeline’s interactions with the component’s Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_op(data):\n",
    "    return dsl.ContainerOp(\n",
    "        name = 'Preprocess Data',\n",
    "        image = 'mavencodev/preprocess-component:v.0.1',\n",
    "        arguments = ['--data', data],\n",
    "        file_outputs={\n",
    "            'X_train': '/preprocess_data/X_train.npy',\n",
    "            'X_test': '/preprocess_data/X_test.npy',\n",
    "            'y_train': '/preprocess_data/y_train.npy',\n",
    "            'y_test': '/preprocess_data/y_test.npy'     \n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using YAML file to create component. To create a yaml file from the python function, a few things are added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_path,train_data,test_data):\n",
    "    import pickle\n",
    "    # import Library\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn==0.22'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas==0.23.4'])\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "    #importing the data\n",
    "    data = pd.read_csv(\"https://raw.githubusercontent.com/MavenCode/KubeflowTraining/master/Data/Churn_Modelling.csv\")\n",
    "\n",
    "    #dropping some columns that are not needed\n",
    "    data = data.drop(columns=['RowNumber','CustomerId','Surname'], axis=1)\n",
    "    #data features\n",
    "    X = data.iloc[:,:-1]\n",
    "    #target data\n",
    "    y = data.iloc[:,-1:]   \n",
    "    #encoding the categorical columns\n",
    "    le = LabelEncoder()\n",
    "    ohe = OneHotEncoder()\n",
    "    X['Gender'] = le.fit_transform(X['Gender'])\n",
    "    geo_df = pd.DataFrame(ohe.fit_transform(X[['Geography']]).toarray())\n",
    "\n",
    "    #getting feature name after onehotencoding\n",
    "    geo_df.columns = ohe.get_feature_names(['Geography'])\n",
    "\n",
    "    #merging geo_df with the main data\n",
    "    X = X.join(geo_df) \n",
    "    #dropping the old columns after encoding\n",
    "    X.drop(columns=['Geography'], axis=1, inplace=True)\n",
    "\n",
    "    #splitting the data \n",
    "    X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.2, random_state = 42)\n",
    "    #feature scaling\n",
    "    sc =StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    #saving the values from the dataframe\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "    \n",
    "    #Save the train_data as a pickle file to be used by the train component.\n",
    "    with open(f'{data_path}/{train_data}', 'wb') as f:\n",
    "        pickle.dump((X_train,  y_train), f)\n",
    "        \n",
    "    #Save the test_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/{test_data}', 'wb') as f:\n",
    "        pickle.dump((X_test,  y_test), f)\n",
    "    \n",
    "    return(print('Done!'))\n",
    "\n",
    "#exporting function to YAML file\n",
    "if __name__ == \"__main__\":\n",
    "    kfp.components.create_component_from_func(\n",
    "    preprocess, #function name\n",
    "    output_component_file=\"preprocess-component.yaml\") ,\n",
    "    base_image=\"python:3.7.1\",\n",
    "    packages_to_install = [\"pandas==0.23.4\", \"scikit-learn==0.22\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The YAML file is created in the working directory of the notebook except if it is specified otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a copy of the YAML file  shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name: Preprocess\n",
    "inputs:\n",
    "- {name: data_path}\n",
    "- {name: train_data}\n",
    "- {name: test_data}\n",
    "implementation:\n",
    "  container:\n",
    "    image: python:3.7\n",
    "    command:\n",
    "    - sh\n",
    "    - -ec\n",
    "    - |\n",
    "      program_path=$(mktemp)\n",
    "      printf \"%s\" \"$0\" > \"$program_path\"\n",
    "      python3 -u \"$program_path\" \"$@\"\n",
    "    - \"def preprocess(data_path,train_data,test_data):\\n    import pickle\\n    # import\\\n",
    "      \\ Library\\n    import sys, subprocess;\\n    subprocess.run([sys.executable,\\\n",
    "      \\ '-m', 'pip', 'install','scikit-learn==0.22'])\\n    subprocess.run([sys.executable,\\\n",
    "      \\ '-m', 'pip', 'install','pandas==0.23.4'])\\n    import pandas as pd\\n    import\\\n",
    "      \\ numpy as np\\n    from sklearn.preprocessing import LabelEncoder\\n    from\\\n",
    "      \\ sklearn.preprocessing import OneHotEncoder\\n    from sklearn.model_selection\\\n",
    "      \\ import train_test_split\\n    from sklearn.preprocessing import StandardScaler\\\n",
    "      \\ \\n\\n    #importing the data\\n    data = pd.read_csv(\\\"https://raw.githubusercontent.com/MavenCode/KubeflowTraining/master/Data/Churn_Modelling.csv\\\"\\\n",
    "      )\\n\\n    #dropping some columns that are not needed\\n    data = data.drop(columns=['RowNumber','CustomerId','Surname'],\\\n",
    "      \\ axis=1)\\n    #data features\\n    X = data.iloc[:,:-1]\\n    #target data\\n\\\n",
    "      \\    y = data.iloc[:,-1:]   \\n    #encoding the categorical columns\\n    le\\\n",
    "      \\ = LabelEncoder()\\n    ohe = OneHotEncoder()\\n    X['Gender'] = le.fit_transform(X['Gender'])\\n\\\n",
    "      \\    geo_df = pd.DataFrame(ohe.fit_transform(X[['Geography']]).toarray())\\n\\n\\\n",
    "      \\    #getting feature name after onehotencoding\\n    geo_df.columns = ohe.get_feature_names(['Geography'])\\n\\\n",
    "      \\n    #merging geo_df with the main data\\n    X = X.join(geo_df) \\n    #dropping\\\n",
    "      \\ the old columns after encoding\\n    X.drop(columns=['Geography'], axis=1,\\\n",
    "      \\ inplace=True)\\n\\n    #splitting the data \\n    X_train,X_test,y_train,y_test\\\n",
    "      \\ = train_test_split( X,y, test_size=0.2, random_state = 42)\\n    #feature scaling\\n\\\n",
    "      \\    sc =StandardScaler()\\n    X_train = sc.fit_transform(X_train)\\n    X_test\\\n",
    "      \\ = sc.transform(X_test)\\n    #saving the values from the dataframe\\n    y_train\\\n",
    "      \\ = y_train.values\\n    y_test = y_test.values\\n\\n    #Save the train_data as\\\n",
    "      \\ a pickle file to be used by the train component.\\n    with open(f'{data_path}/{train_data}',\\\n",
    "      \\ 'wb') as f:\\n        pickle.dump((X_train,  y_train), f)\\n\\n    #Save the\\\n",
    "      \\ test_data as a pickle file to be used by the predict component.\\n    with\\\n",
    "      \\ open(f'{data_path}/{test_data}', 'wb') as f:\\n        pickle.dump((X_test,\\\n",
    "      \\  y_test), f)\\n\\n    return(print('Done!'))\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog='Preprocess',\\\n",
    "      \\ description='')\\n_parser.add_argument(\\\"--data-path\\\", dest=\\\"data_path\\\"\\\n",
    "      , type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\\"\\\n",
    "      --train-data\\\", dest=\\\"train_data\\\", type=str, required=True, default=argparse.SUPPRESS)\\n\\\n",
    "      _parser.add_argument(\\\"--test-data\\\", dest=\\\"test_data\\\", type=str, required=True,\\\n",
    "      \\ default=argparse.SUPPRESS)\\n_parsed_args = vars(_parser.parse_args())\\n\\n\\\n",
    "      _outputs = preprocess(**_parsed_args)\\n\"\n",
    "    args:\n",
    "    - --data-path\n",
    "    - {inputValue: data_path}\n",
    "    - --train-data\n",
    "    - {inputValue: train_data}\n",
    "    - --test-data\n",
    "    - {inputValue: test_data}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading the yaml file from the preprocess function.\n",
    "##### kfp.components.load_component_from_file or kfp.components.load_component_from_url  can be used to load the yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Preprocess(data_path, train_data, test_data)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfp.components.load_component_from_file(\"preprocess-component.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The different ways of creating pipeline components have been discussed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

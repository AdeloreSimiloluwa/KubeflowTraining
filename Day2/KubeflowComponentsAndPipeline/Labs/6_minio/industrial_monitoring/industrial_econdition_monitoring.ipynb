{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Industrial Equipment Monitoring\n",
    "\n",
    "## Introduction\n",
    "[Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/) helps with building entire workflows\n",
    "\n",
    "These steps can be triggered automatically by a CI/CD workflow or on demand from a command line or notebook.\n",
    "\n",
    "\n",
    "**Components** performs a single step in a Machine Learning workflow such (e.g. data ingestion, data preprocessing, data transformation, model training, hyperparameter tuning). \n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "check to see if kfp is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 show kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure access Minio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload our Training Dataset to Minio\n",
    "\n",
    "First, we configure credentials for `mc`, the MinIO command line client.\n",
    "We then use it to create a bucket, upload the dataset to it, and set access policy so that the pipeline can download it from MinIO.\n",
    "\n",
    "Follow the steps below to download minio client\n",
    "<div class=\"alert\">\n",
    "   <code>\n",
    "    wget https://dl.min.io/client/mc/release/linux-amd64/mc\n",
    "    chmod +x mc\n",
    "    ./mc --help\n",
    "    </code>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://dl.min.io/client/mc/release/linux-amd64/mc\n",
    "! chmod +x mc\n",
    "! ./mc --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Connect to the Minio Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./mc alias set minio http://minio-service.kubeflow:9000 minio minio123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Create a bucket to stor your data and export your model to Minio\n",
    "\n",
    "**Make sure you clear this bucket once you are cone running your pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./mc mb minio/monitoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./mc ls minio mlpipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Upload the dataset to your bucket in Minio.\n",
    "\n",
    "**Note**: Make sure you have your dataset in a folder like we have here as <code>datasets</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar --dereference -czf datasets.tar.gz ./datasets\n",
    "! ./mc cp datasets.tar.gz minio/monitoring/datasets.tar.gz\n",
    "! ./mc policy set download minio/monitoring "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you have downloaded your data too many times while testing, use the following code to clear out your bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ./mc rm --recursive --force minio/monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minio Server URL and Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_SERVER='minio-service.kubeflow:9000'\n",
    "MINIO_ACCESS_KEY='minio'\n",
    "MINIO_SECRET_KEY='minio123'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Implement Kubeflow Pipelines Components\n",
    "\n",
    "In this pipeline, we have the following components:\n",
    "- Sensor data download component\n",
    "- Preprocess the dataset component\n",
    "- Train the model component\n",
    "- Test model component\n",
    "- Print Results component\n",
    "- Export the trained model component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import kfp\n",
    "import kfp.components as components\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import InputPath, OutputPath #helps define the input & output between the components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 1: Download the  Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(minio_server: str, data_dir: OutputPath(str)):\n",
    "    \"\"\"Download the data set to the KFP volume to share it among all steps\"\"\"\n",
    "    import urllib.request\n",
    "    import tarfile\n",
    "    import os\n",
    "    import subprocess\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    url = f'http://{minio_server}/monitoring/datasets.tar.gz'\n",
    "    print(url)\n",
    "    stream = urllib.request.urlopen(url)\n",
    "    print('done downloading')\n",
    "    tar = tarfile.open(fileobj=stream, mode=\"r|gz\")\n",
    "    tar.extractall(path=data_dir)\n",
    "    print('done extracting')\n",
    "    \n",
    "    \n",
    "    subprocess.call([\"ls\", \"-dlha\", data_dir])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 2: Preprocess the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data_dir: InputPath(str), clean_data_dir: OutputPath(str)):\n",
    "    \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'sklearn'])\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import pickle\n",
    "    from sklearn import preprocessing\n",
    "    \n",
    "    df = pd.read_csv(f\"{data_dir}/datasets/data.csv\")\n",
    "    df = df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp','mode', 'pCut::Motor_Torque'\n",
    "                 ,'pCut::CTRL_Position_controller::Lag_error','pSvolFilm::CTRL_Position_controller::Lag_error'], axis=1)\n",
    "    df = df.fillna(0)\n",
    "    train_percentage = 0.30\n",
    "    train_size = int(len(df.index)*train_percentage)\n",
    "    x_train = df[:train_size]\n",
    "    x_test = df[train_size:490000]\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(x_train), \n",
    "                                columns=x_train.columns, \n",
    "                                index=x_train.index)\n",
    "    # Random shuffle training data\n",
    "    X_train.sample(frac=1)\n",
    "\n",
    "    X_test = pd.DataFrame(scaler.transform(x_test), \n",
    "                              columns=x_test.columns, \n",
    "                              index=x_test.index)\n",
    "    data = {\"X_train\": X_train,\"X_test\": X_test}\n",
    "    \n",
    "    \n",
    "    os.makedirs(clean_data_dir, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(clean_data_dir,'clean_data.pickle'), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    print(f\"clean_data.pickle {clean_data_dir}\")\n",
    "    \n",
    "    print(os.listdir(clean_data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 3: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(clean_data_dir: InputPath(str), model_dir: OutputPath(str)):\n",
    "    #importing libraries\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'tensorflow'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'keras'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'joblib'])\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import os\n",
    "    import joblib\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from keras.layers import Input, Dropout\n",
    "    from keras.layers.core import Dense \n",
    "    from keras.models import Model, Sequential, load_model\n",
    "    from keras import regularizers\n",
    "\n",
    "    with open(os.path.join(clean_data_dir,'clean_data.pickle'), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        \n",
    "    print(data) \n",
    "    \n",
    "    np.random.seed(10)\n",
    "    tf.random.set_seed(10)\n",
    "    X_train = data['X_train']\n",
    "    act_func = 'elu'\n",
    "\n",
    "    # Input layer:\n",
    "    model=Sequential()\n",
    "    # First hidden layer, connected to input vector X. \n",
    "    model.add(Dense(10,activation=act_func,\n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  kernel_regularizer=regularizers.l2(0.0),\n",
    "                  input_shape=(X_train.shape[1],)\n",
    "                )\n",
    "          )\n",
    "\n",
    "    model.add(Dense(2,activation=act_func,\n",
    "                  kernel_initializer='glorot_uniform'))\n",
    "\n",
    "    model.add(Dense(10,activation=act_func,\n",
    "                  kernel_initializer='glorot_uniform'))\n",
    "\n",
    "    model.add(Dense(X_train.shape[1],\n",
    "                  kernel_initializer='glorot_uniform'))\n",
    "\n",
    "    model.compile(loss='mse',optimizer='adam')\n",
    "\n",
    "    # Train model for 100 epochs, batch size of 10: \n",
    "    NUM_EPOCHS=5\n",
    "    BATCH_SIZE=10\n",
    "\n",
    "    model.fit(np.array(X_train),np.array(X_train),\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_split=0.1,\n",
    "                    verbose = 1)\n",
    "    \n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    model.save(model_dir + \"/model.h5\")\n",
    "    print(f\"Model saved {model_dir}\")\n",
    "    print(os.listdir(model_dir))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 4: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(clean_data_dir: InputPath(str), pca_dir: OutputPath(str)):\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'sklean'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'joblib'])\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pickle\n",
    "    import joblib\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    with open(os.path.join(clean_data_dir,'clean_data.pickle'), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        \n",
    "    print(data)\n",
    "    \n",
    "    \n",
    "    X_train = data['X_train']\n",
    "    X_test = data['X_test']  \n",
    "    print(len(X_train),len(X_test))\n",
    "    pca = PCA(n_components=2, svd_solver= 'full')\n",
    "    X_train_PCA = pca.fit_transform(X_train)\n",
    "    X_train_PCA = pd.DataFrame(X_train_PCA)\n",
    "    X_train_PCA.index = X_train.index\n",
    "\n",
    "    X_test_PCA = pca.transform(X_test)\n",
    "    X_test_PCA = pd.DataFrame(X_test_PCA)\n",
    "    X_test_PCA.index = X_test.index\n",
    "    \n",
    "    def is_pos_def(A):\n",
    "\n",
    "        if np.allclose(A, A.T):\n",
    "            try:\n",
    "                np.linalg.cholesky(A)\n",
    "                return True\n",
    "            except np.linalg.LinAlgError:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def cov_matrix(data, verbose=False):\n",
    "        covariance_matrix = np.cov(data, rowvar=False)\n",
    "        \n",
    "        if is_pos_def(covariance_matrix):\n",
    "            inv_covariance_matrix = np.linalg.inv(covariance_matrix)\n",
    "            if is_pos_def(inv_covariance_matrix):\n",
    "                return covariance_matrix, inv_covariance_matrix\n",
    "            else:\n",
    "                print(\"Error: Inverse of Covariance Matrix is not positive definite!\")\n",
    "        else:\n",
    "            print(\"Error: Covariance Matrix is not positive definite!\")\n",
    "    def MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):\n",
    "        inv_covariance_matrix = inv_cov_matrix\n",
    "        vars_mean = mean_distr\n",
    "        diff = data - vars_mean\n",
    "        md = []\n",
    "        for i in range(len(diff)):\n",
    "            md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))\n",
    "        return md\n",
    "\n",
    "    def MD_threshold(dist, extreme=False, verbose=False):\n",
    "        k = 3. if extreme else 2.\n",
    "        threshold = np.mean(dist) * k\n",
    "        return threshold\n",
    "\n",
    "    def is_pos_def(A):\n",
    "\n",
    "        if np.allclose(A, A.T):\n",
    "            try:\n",
    "                np.linalg.cholesky(A)\n",
    "                return True\n",
    "            except np.linalg.LinAlgError:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    data_train = np.array(X_train_PCA.values)\n",
    "    data_test = np.array(X_test_PCA.values)\n",
    "    cov_matrix, inv_cov_matrix  = cov_matrix(data_train)\n",
    "    mean_distr = data_train.mean(axis=0)\n",
    "    dist_test = MahalanobisDist(inv_cov_matrix, mean_distr, data_test, verbose=False)\n",
    "    dist_train = MahalanobisDist(inv_cov_matrix, mean_distr, data_train, verbose=False)\n",
    "    threshold = MD_threshold(dist_train, extreme = True)\n",
    "\n",
    "    anomaly_train = pd.DataFrame()\n",
    "    anomaly_train['Mob dist']= dist_train\n",
    "    anomaly_train['Thresh'] = threshold\n",
    "    # If Mob dist above threshold: Flag as anomaly\n",
    "    anomaly_train['Anomaly'] = anomaly_train['Mob dist'] > anomaly_train['Thresh']\n",
    "    anomaly_train.index = X_train_PCA.index\n",
    "\n",
    "    anomaly = pd.DataFrame()\n",
    "    anomaly['Mob dist']= dist_test\n",
    "    anomaly['Thresh'] = threshold\n",
    "    # If Mob dist above threshold: Flag as anomaly\n",
    "    anomaly['Anomaly'] = anomaly['Mob dist'] > anomaly['Thresh']\n",
    "    anomaly.index = X_test_PCA.index\n",
    "\n",
    "\n",
    "    anomaly_alldata = pd.concat([anomaly_train, anomaly])\n",
    "    data = anomaly_alldata[anomaly_alldata['Anomaly']==True]\n",
    "\n",
    "    \n",
    "    os.makedirs(pca_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(pca_dir,'pca_metrics.pickle'), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    print(f\"pca_metrics.pickle {pca_dir}\")\n",
    "    \n",
    "    print(os.listdir(pca_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 5: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(clean_data_dir: InputPath(str), model_dir: InputPath(str), metrics_path: OutputPath(str)):\n",
    "    \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'tensorflow'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'keras'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'joblib'])\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from tensorflow import keras\n",
    "    from keras.models import load_model\n",
    "    import os\n",
    "    import joblib\n",
    "    import pickle\n",
    "    \n",
    "    print(clean_data_dir)\n",
    "    with open(os.path.join(clean_data_dir,'clean_data.pickle'), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        \n",
    "    print(data)\n",
    "    \n",
    "    model = keras.models.load_model(model_dir + \"/model.h5\")      \n",
    "    print(model)\n",
    "    \n",
    "    \n",
    "    X_train = data['X_train']\n",
    "    X_test = data['X_test']\n",
    "    X_pred_train = model.predict(np.array(X_train))\n",
    "    X_pred_train = pd.DataFrame(X_pred_train, \n",
    "                        columns=X_train.columns)\n",
    "    X_pred_train.index = X_train.index\n",
    "\n",
    "    scored_train = pd.DataFrame(index=X_train.index)\n",
    "    scored_train['Loss_mae'] = np.mean(np.abs(X_pred_train-X_train), axis = 1)\n",
    "    thress = np.max(scored_train['Loss_mae'])\n",
    "    scored_train['Threshold'] = thress\n",
    "    scored_train['Anomaly'] = scored_train['Loss_mae'] > scored_train['Threshold']\n",
    "    X_pred = model.predict(np.array(X_test))\n",
    "    X_pred = pd.DataFrame(X_pred, \n",
    "                        columns=X_test.columns)\n",
    "    X_pred.index = X_test.index\n",
    "\n",
    "    scored = pd.DataFrame(index=X_test.index)\n",
    "    scored = scored[:1000]\n",
    "    scored['Loss_mae'] = np.mean(np.abs(X_pred-X_test), axis = 1)\n",
    "    scored['Threshold'] = thress\n",
    "    scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\n",
    "    scored = pd.concat([scored_train, scored])\n",
    "    data = scored[scored['Anomaly']==True] \n",
    "    \n",
    "    print(data)\n",
    "    \n",
    "    os.makedirs(metrics_path, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(metrics_path,'metrics.pickle'), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    print(f\"metrics.pickle {metrics_path}\")\n",
    "    \n",
    "    print(os.listdir(metrics_path))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 6: Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(pca_dir: InputPath(str), metrics_path: InputPath(str)):\n",
    "    \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas'])\n",
    "    import pickle\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    with open(os.path.join(metrics_path,'metrics.pickle'), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        \n",
    "    print(data)\n",
    "    print(\"Autoencoder\")\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        print(f\"There are anomalies in the data, {len(data)} \\n\\n\")\n",
    "        print(data.head(20))\n",
    "    else:\n",
    "        \n",
    "        print(f\"There are no anomalies\")\n",
    "        print(\"\\n\\n **************** \\n\\n\")\n",
    "        \n",
    "    with open(os.path.join(pca_dir,'pca_metrics.pickle'), 'rb') as f:\n",
    "        data1 = pickle.load(f)\n",
    "        \n",
    "    print(data1) \n",
    "    print(\"PCA\")\n",
    "    \n",
    "    if len(data1) > 0:\n",
    "        print(f\"There are anomalies in the data, {len(data1)} \\n\\n\")\n",
    "        print(data1.head(20))\n",
    "    else:\n",
    "        print(f\"There are no anomalies\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 7: Export the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(\n",
    "    model_dir: InputPath(str),\n",
    "    metrics_path: str,\n",
    "    export_bucket: str,\n",
    "    model_name: str,\n",
    "    model_version: int,\n",
    "    minio_server: str,\n",
    "    minio_access_key: str,\n",
    "    minio_secret_key: str,\n",
    "):\n",
    "    import os\n",
    "    import boto3\n",
    "    from botocore.client import Config\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=f'http://{minio_server}',\n",
    "        aws_access_key_id=minio_access_key,\n",
    "        aws_secret_access_key=minio_secret_key,\n",
    "        config=Config(signature_version=\"s3v4\"),\n",
    "    )\n",
    "\n",
    "    # Create export bucket if it does not yet exist\n",
    "    response = s3.list_buckets()\n",
    "    export_bucket_exists = False\n",
    "\n",
    "    for bucket in response[\"Buckets\"]:\n",
    "        if bucket[\"Name\"] == export_bucket:\n",
    "            export_bucket_exists = True\n",
    "\n",
    "    if not export_bucket_exists:\n",
    "        s3.create_bucket(ACL=\"public-read-write\", Bucket=export_bucket)\n",
    "\n",
    "    # Save model files to S3\n",
    "    for root, dirs, files in os.walk(model_dir):\n",
    "        for filename in files:\n",
    "            local_path = os.path.join(root, filename)\n",
    "            s3_path = os.path.relpath(local_path, model_dir)\n",
    "\n",
    "            s3.upload_file(\n",
    "                local_path,\n",
    "                export_bucket,\n",
    "                f\"{model_name}/{model_version}/{s3_path}\",\n",
    "                ExtraArgs={\"ACL\": \"public-read\"},\n",
    "            )\n",
    "\n",
    "    response = s3.list_objects(Bucket=export_bucket)\n",
    "    print(f\"All objects in {export_bucket}:\")\n",
    "    for file in response[\"Contents\"]:\n",
    "        print(\"{}/{}\".format(export_bucket, file[\"Key\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the Components into a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_pipeline(\n",
    "    data_dir: str,\n",
    "    clean_data_dir: str,\n",
    "    metrics_path:str,\n",
    "    pca_dir: str,\n",
    "    model_dir: str,\n",
    "    export_bucket: str,\n",
    "    model_name: str,\n",
    "    model_version: int,\n",
    "    minio_server: str,\n",
    "    minio_access_key: str,\n",
    "    minio_secret_key: str,\n",
    "):\n",
    "    # For GPU support, please add the \"-gpu\" suffix to the base image\n",
    "    BASE_IMAGE = \"mavencodev/minio:v.0.1\"\n",
    "\n",
    "    downloadOp = components.func_to_container_op(\n",
    "        download_dataset, base_image=BASE_IMAGE\n",
    "    )(minio_server)\n",
    "\n",
    "    preprocessOp = components.func_to_container_op(preprocessing, base_image=BASE_IMAGE)(\n",
    "        downloadOp.output\n",
    "    )\n",
    "    \n",
    "    trainOp = components.func_to_container_op(train, base_image=BASE_IMAGE)(\n",
    "        preprocessOp.output\n",
    "    )\n",
    "    \n",
    "    pcaOp = components.func_to_container_op(PCA, base_image=BASE_IMAGE)(\n",
    "        preprocessOp.output\n",
    "    )\n",
    "\n",
    "    testOp = components.func_to_container_op(test, base_image=BASE_IMAGE)(\n",
    "        preprocessOp.output, trainOp.output\n",
    "    )\n",
    "    \n",
    "    resultOp = components.func_to_container_op(results, base_image=BASE_IMAGE)(\n",
    "        testOp.output, pcaOp.output\n",
    "    )\n",
    "\n",
    "    exportOp = components.func_to_container_op(export_model, base_image=BASE_IMAGE)(\n",
    "        trainOp.output, testOp.output, export_bucket, model_name, model_version,\n",
    "        minio_server, minio_access_key, minio_secret_key\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def op_transformer(op):\n",
    "    op.add_pod_annotation(name=\"sidecar.istio.io/inject\", value=\"false\")\n",
    "    return op\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"End-to-End Industrial IoT Pipeline\",\n",
    "    description=\"A sample pipeline to demonstrate multi-step model training, evaluation and export\",\n",
    ")\n",
    "def monitoring_pipeline(\n",
    "    model_dir: str = \"/train/model\",\n",
    "    data_dir: str = \"/train/data\",\n",
    "    clean_data_dir: str = \"/train/data\",\n",
    "    metrics_path: str =\"/train/metrics\",\n",
    "    pca_dir: str =\"/train/metrics\",\n",
    "    export_bucket: str = \"monitoring\",\n",
    "    model_name: str = \"monitoring\",\n",
    "    model_version: int = 1,\n",
    "):\n",
    "    MINIO_SERVER='minio-service.kubeflow:9000'\n",
    "    MINIO_ACCESS_KEY='minio'\n",
    "    MINIO_SECRET_KEY='minio123'\n",
    "    \n",
    "    train_model_pipeline(\n",
    "        data_dir=data_dir,\n",
    "        pca_dir=pca_dir,\n",
    "        metrics_path = metrics_path,\n",
    "        clean_data_dir=clean_data_dir,\n",
    "        model_dir=model_dir,\n",
    "        export_bucket=export_bucket,\n",
    "        model_name=model_name,\n",
    "        model_version=model_version,\n",
    "        minio_server=MINIO_SERVER,\n",
    "        minio_access_key=MINIO_ACCESS_KEY,\n",
    "        minio_secret_key=MINIO_SECRET_KEY,\n",
    "    )\n",
    "    \n",
    "    dsl.get_pipeline_conf().add_op_transformer(op_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place, let's submit the pipeline directly from our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = monitoring_pipeline\n",
    "run_name = pipeline_func.__name__ + \" run\"\n",
    "experiment_name = \"End-to-End Industrial IOT Kubeflow\"\n",
    "\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  'monitoring.yaml')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the generated yaml file to create a pipeline in Kubeflow UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now delete your bucket when you have run the pipeline successfully in the Kubeflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ./mc rb minio/monitoring--force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
